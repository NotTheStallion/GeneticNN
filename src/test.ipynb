{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label shape:  torch.Size([32])\n",
      "First 5 training labels:  tensor([4, 3, 3, 9, 5])\n",
      "tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])\n",
      "784\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 784])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup train and test splits\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "print(\"Training label shape: \", labels.shape)  # (32,) -- 32 numbers (all 0-9)\n",
    "print(\"First 5 training labels: \", labels[:5])  # [5, 0, 4, 1, 9]\n",
    "\n",
    "# Convert to \"one-hot\" vectors\n",
    "num_classes = 10\n",
    "y_train = torch.nn.functional.one_hot(labels, num_classes=num_classes)\n",
    "print(y_train[:5])\n",
    "\n",
    "# splitting dataset for faster learning and debugging\n",
    "X_train, y_train = images[:10000], y_train[:10000]\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "\n",
    "# input and output shape\n",
    "input_shape = X_train[1].shape\n",
    "num_classes = 10\n",
    "# Flatten the images\n",
    "image_vector_size = 28 * 28\n",
    "print(image_vector_size)\n",
    "X_train = X_train.view(X_train.size(0), -1)\n",
    "X_test = X_test.view(X_test.size(0), -1)\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "# DNA[0] = depth\n",
    "# DNA[1] = neurons_per_layer\n",
    "# DNA[2] = activations\n",
    "# DNA[3] = optimizer\n",
    "# DNA[4] = losses\n",
    "DNA_parameter = [[5, 6, 7, 8, 9, 10],\n",
    "                 [16, 32, 64, 128, 256, 512, 1024],\n",
    "                 [\"tanh\", \"softmax\", \"relu\", \"sigmoid\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"hard_sigmoid\", \"linear\"],\n",
    "                 [\"sgd\", \"rmsprop\", \"adagrad\", \"adadelta\", \"adam\", \"adamax\", \"nadam\"],\n",
    "                 [\"mse_loss\", \"l1_loss\", \"cross_entropy\", \"nll_loss\", \"poisson_nll_loss\", \"kl_div_loss\", \"bce_loss\", \"bce_with_logits_loss\"]\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape, classes, DNA_param, epochs):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.architecture_DNA = []  # to save current parameters\n",
    "        self.fitness = []\n",
    "        self.acc_history = []\n",
    "        self.input_shape = input_shape\n",
    "        self.classes = classes\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # unfold DNA_parameters:\n",
    "        depth = DNA_param[0]\n",
    "        neurons_per_layer = DNA_param[1]\n",
    "        activations = DNA_param[2]\n",
    "        optimizers = DNA_param[3]\n",
    "        losses = DNA_param[4]\n",
    "\n",
    "        layers = []\n",
    "        network_depth = np.random.choice(depth)\n",
    "        self.architecture_DNA.append(network_depth)\n",
    "\n",
    "        for i in range(network_depth):\n",
    "            if i == 0:\n",
    "                neurons = np.random.choice(neurons_per_layer)\n",
    "                activation = np.random.choice(activations)\n",
    "                self.architecture_DNA.append([neurons, activation])\n",
    "                layers.append(nn.Linear(self.input_shape, neurons))\n",
    "                layers.append(self.get_activation(activation))\n",
    "            elif i == network_depth - 1:\n",
    "                activation = np.random.choice(activations)\n",
    "                self.architecture_DNA.append(activation)\n",
    "                layers.append(nn.Linear(neurons, self.classes))\n",
    "                layers.append(self.get_activation(activation))\n",
    "            else:\n",
    "                neurons = np.random.choice(neurons_per_layer)\n",
    "                activation = np.random.choice(activations)\n",
    "                self.architecture_DNA.append([neurons, activation])\n",
    "                layers.append(nn.Linear(neurons, neurons))\n",
    "                layers.append(self.get_activation(activation))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        loss = np.random.choice(losses)\n",
    "        optimizer = np.random.choice(optimizers)\n",
    "        self.architecture_DNA.append([loss, optimizer])\n",
    "        self.loss_fn = self.get_loss(loss)\n",
    "        self.optimizer = self.get_optimizer(optimizer, self.model.parameters())\n",
    "\n",
    "    def get_activation(self, activation):\n",
    "        activations = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'softmax': nn.Softmax(dim=1),\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'selu': nn.SELU(),\n",
    "            'softplus': nn.Softplus(),\n",
    "            'softsign': nn.Softsign(),\n",
    "            'hard_sigmoid': nn.Hardsigmoid(),\n",
    "            'linear': nn.Identity()\n",
    "        }\n",
    "        return activations[activation]\n",
    "\n",
    "    def get_loss(self, loss):\n",
    "        losses = {\n",
    "            'mse_loss': nn.MSELoss(),\n",
    "            'l1_loss': nn.L1Loss(),\n",
    "            'cross_entropy': nn.CrossEntropyLoss(),\n",
    "            'nll_loss': nn.NLLLoss(),\n",
    "            'poisson_nll_loss': nn.PoissonNLLLoss(),\n",
    "            'kl_div_loss': nn.KLDivLoss(),\n",
    "            'bce_loss': nn.BCELoss(),\n",
    "            'bce_with_logits_loss': nn.BCEWithLogitsLoss()\n",
    "        }\n",
    "        return losses[loss]\n",
    "\n",
    "    def get_optimizer(self, optimizer, parameters):\n",
    "        optimizers = {\n",
    "            'sgd': optim.SGD(parameters, lr=0.01),\n",
    "            'rmsprop': optim.RMSprop(parameters, lr=0.01),\n",
    "            'adagrad': optim.Adagrad(parameters, lr=0.01),\n",
    "            'adadelta': optim.Adadelta(parameters, lr=0.01),\n",
    "            'adam': optim.Adam(parameters, lr=0.01),\n",
    "            'adamax': optim.Adamax(parameters, lr=0.01),\n",
    "            'nadam': optim.NAdam(parameters, lr=0.01)\n",
    "        }\n",
    "        return optimizers[optimizer]\n",
    "\n",
    "    def create_children(self, children_DNA):\n",
    "        layers = []\n",
    "        children_depth = children_DNA[0]\n",
    "        for i in range(children_depth):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(self.input_shape, children_DNA[1][0]))\n",
    "                layers.append(self.get_activation(children_DNA[1][1]))\n",
    "            elif i == children_depth - 1:\n",
    "                layers.append(nn.Linear(children_DNA[i][0], self.classes))\n",
    "                layers.append(self.get_activation(children_DNA[children_depth]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(children_DNA[i][0], children_DNA[i+1][0]))\n",
    "                layers.append(self.get_activation(children_DNA[i+1][1]))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.loss_fn = self.get_loss(children_DNA[-1][0])\n",
    "        self.optimizer = self.get_optimizer(children_DNA[-1][1], self.model.parameters())\n",
    "        self.architecture_DNA = children_DNA\n",
    "\n",
    "    def give_fitness(self):\n",
    "        return self.fitness\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X_train)\n",
    "            loss = self.loss_fn(outputs, y_train)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_test)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == y_test).sum().item()\n",
    "            self.fitness = correct / len(y_test)\n",
    "            self.acc_history.append(self.fitness)\n",
    "\n",
    "    def give_DNA(self):\n",
    "        return self.architecture_DNA\n",
    "\n",
    "    def architecture(self):\n",
    "        print(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAlgorithm:\n",
    "    def __init__(self, population_size, mutation_rate, generations = 50, Epochs = 2):\n",
    "        self.population_size = population_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.generations = generations\n",
    "        self.training_epochs = Epochs\n",
    "        self.population = None\n",
    "        self.children_population_DNA = []\n",
    "        self.acces = []\n",
    "        self.norm_acces = []\n",
    "        \n",
    "    def create_population(self):\n",
    "        self.population = [Network(image_vector_size, num_classes, DNA_parameter,self.training_epochs) for i in range(self.population_size)]\n",
    "    \n",
    "    def train_generation(self):\n",
    "        for member in self.population:\n",
    "                member.train()\n",
    "                \n",
    "    def predict(self):\n",
    "        for member in self.population:\n",
    "                member.test()\n",
    "                self.acc.append(member.give_fitness())\n",
    "    \n",
    "    def normalize(self):\n",
    "        sum_ = sum(self.acc)\n",
    "        self.norm_acc = [i/sum_ for i in self.acc] \n",
    "        #print(\"\\nNormalization sum: \",sum(self.norm_acc))\n",
    "        #assert sum(self.norm_acc) == 1\n",
    "        \n",
    "    def clear_losses(self):\n",
    "        self.norm_acc = []\n",
    "        self.acc = []\n",
    "        \n",
    "    def mutate(self):\n",
    "        for child_DNA in self.children_population_DNA:\n",
    "            for i in range(len(child_DNA)):\n",
    "                if np.random.random() < self.mutation_rate:\n",
    "                    print(\"\\nMutation!\")\n",
    "                    if i == 0:\n",
    "                        new_depth = np.random.choice(DNA_parameter[0])\n",
    "                        child_DNA[0] = new_depth\n",
    "                    \n",
    "                    if i == len(child_DNA)-2:\n",
    "                        new_output_activation = np.random.choice(DNA_parameter[2])\n",
    "                        child_DNA[-2] = new_output_activation\n",
    "                    \n",
    "                    if i == len(child_DNA)-1:\n",
    "                        # random flip if loss or activation shall be changed\n",
    "                        if np.random.random() < 0.5:\n",
    "                            new_loss = np.random.choice(DNA_parameter[4])\n",
    "                            child_DNA[-1][0] = new_loss\n",
    "                        else:\n",
    "                            new_optimizer = np.random.choice(DNA_parameter[3])\n",
    "                            child_DNA[-1][1] = new_optimizer\n",
    "                    if i != 0 and i !=len(child_DNA)-2 and i != len(child_DNA)-1:\n",
    "                    #else:\n",
    "                        # 3/2 flif if number of neurons or activation function mutates:\n",
    "                        #print(child_DNA)\n",
    "                        if np.random.random() < 0.33:\n",
    "                            #print(child_DNA[i][1])\n",
    "                            new_activation = np.random.choice(DNA_parameter[2])\n",
    "                            #print(new_activation)\n",
    "                            child_DNA[i][1] = new_activation\n",
    "                        else:\n",
    "                            #print(child_DNA[i][0])\n",
    "                            new_neuron_count = np.random.choice(DNA_parameter[1])\n",
    "                            child_DNA[i][0] = new_neuron_count\n",
    "                            #print(new_neuron_count)\n",
    "                    #print(\"After mutation \", child_DNA)\n",
    "\n",
    "    def reproduction(self):\n",
    "        \"\"\" \n",
    "        Reproduction through midpoint crossover method \n",
    "        \"\"\"\n",
    "        population_idx = [i for i in range(len(self.population))]\n",
    "        for i in range(len(self.population)):\n",
    "        #selects two parents probabilistic accroding to the fitness score\n",
    "            if sum(self.norm_acc) != 0:\n",
    "                parent1 = np.random.choice(population_idx, p = self.norm_acc)\n",
    "                parent2 = np.random.choice(population_idx, p = self.norm_acc)\n",
    "            else:\n",
    "              # if there are no \"best\" parents choose randomly \n",
    "                parent1 = np.random.choice(population_idx)\n",
    "                parent2 = np.random.choice(population_idx)\n",
    "\n",
    "            # picking random midpoint for crossing over name/DNA\n",
    "            parent1_DNA = self.population[parent1].give_DNA()\n",
    "            parent2_DNA = self.population[parent2].give_DNA()\n",
    "            #print(parent1_DNA)\n",
    "            \n",
    "            mid_point_1 = np.random.choice([i for i in range(2,len(parent1_DNA)-2)])\n",
    "            mid_point_2 = np.random.choice([i for i in range(2,len(parent2_DNA)-2)])\n",
    "            # adding DNA-Sequences of the parents to final DNA\n",
    "            child_DNA = parent1_DNA[:mid_point_1] + parent2_DNA[mid_point_2:]\n",
    "            new_nn_depth = len(child_DNA)-2 # minus 2 because of depth parameter[0] and loss parameter[-1]\n",
    "            child_DNA[0] = new_nn_depth\n",
    "            self.children_population_DNA.append(child_DNA)\n",
    "        # old population gets the new and proper weights\n",
    "        self.mutate()\n",
    "        # keras.backend.clear_session() ## delete old models to free memory\n",
    "        for i in range(len(self.population)):\n",
    "            self.population[i].create_children(self.children_population_DNA[i])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run_evolution(self):\n",
    "        for episode in range(self.generations):\n",
    "            print(\"\\n--- Generation {} ---\".format(episode))\n",
    "            self.clear_losses()\n",
    "            self.train_generation()\n",
    "            self.predict()\n",
    "            if episode != self.generations -1:\n",
    "                self.normalize()\n",
    "                self.reproduction()\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            self.children_population_DNA = []\n",
    "        # plotting history:\n",
    "        for a in range(self.generations):\n",
    "            for member in self.population:\n",
    "                plt.plot(member.acc_history)\n",
    "        plt.xlabel(\"Generations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m GA \u001b[38;5;241m=\u001b[39m GeneticAlgorithm(population_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,mutation_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.03\u001b[39m, generations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,Epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mGA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m GA\u001b[38;5;241m.\u001b[39mrun_evolution()\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mGeneticAlgorithm.create_population\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_population\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation \u001b[38;5;241m=\u001b[39m [\u001b[43mNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_vector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDNA_parameter\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_epochs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_size)]\n",
      "Cell \u001b[0;32mIn[16], line 54\u001b[0m, in \u001b[0;36mNetwork.__init__\u001b[0;34m(self, input_shape, classes, DNA_param, epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture_DNA\u001b[38;5;241m.\u001b[39mappend([loss, optimizer])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss(loss)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 87\u001b[0m, in \u001b[0;36mNetwork.get_optimizer\u001b[0;34m(self, optimizer, parameters)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_optimizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, parameters):\n\u001b[1;32m     85\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mSGD(parameters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRMSprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madagrad\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mAdagrad(parameters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madadelta\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mAdadelta(parameters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mAdam(parameters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madamax\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mAdamax(parameters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mNAdam(parameters, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     93\u001b[0m     }\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimizers[optimizer]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/optim/rmsprop.py:68\u001b[0m, in \u001b[0;36mRMSprop.__init__\u001b[0;34m(self, params, lr, alpha, eps, weight_decay, momentum, centered, capturable, foreach, maximize, differentiable)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid alpha value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     57\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     58\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mmomentum,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m     67\u001b[0m )\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/optim/optimizer.py:366\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    364\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "GA = GeneticAlgorithm(population_size = 4,mutation_rate = 0.03, generations = 6,Epochs=1)\n",
    "GA.create_population()\n",
    "GA.run_evolution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
